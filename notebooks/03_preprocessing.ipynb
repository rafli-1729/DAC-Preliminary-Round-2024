{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a9711e",
   "metadata": {},
   "source": [
    "# üè≠ **Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a9953a",
   "metadata": {},
   "source": [
    "# üìö **Library and Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1968af3",
   "metadata": {},
   "source": [
    "This section initializes the environment and loads the essential libraries used throughout the preprocessing workflow. It first extends sys.path to allow importing project modules, then suppresses warnings to keep the notebook clean. Core tools such as `numpy`, `pandas`, `Pathlib`, and the `KMeans` clustering algorithm are imported, along with the custom preprocessing utilities from src.preprocessing. A small reload helper is defined to streamline development when updating the module. The notebook then sets standardized paths to the processed train and test files, including the engineered versions that will be created later. A final message confirms that all libraries and configuration steps¬†are¬†ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library and configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# System & Environment Configuration\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Ignore warning\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# Core Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Engineering library\n",
    "from sklearn.cluster import KMeans\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Source helper\n",
    "import src.preprocessing as preprocessing\n",
    "\n",
    "# Reload shortcut\n",
    "def r(module=preprocessing):\n",
    "    importlib.reload(module)\n",
    "\n",
    "r()\n",
    "\n",
    "# Train and Test Processed\n",
    "PROCESSED_ROOT = Path('../data/processed/')\n",
    "\n",
    "TRAIN_PATH_PROCESSED = PROCESSED_ROOT/'train.csv'\n",
    "TEST_PATH_PROCESSED = PROCESSED_ROOT/'test.csv'\n",
    "\n",
    "TRAIN_PATH_ENGINEERED = PROCESSED_ROOT/'train_engineered.csv'\n",
    "TEST_PATH_ENGINEERED = PROCESSED_ROOT/'test_engineered.csv'\n",
    "\n",
    "print('library and configuration ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6616f5",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è **Train and Test Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d82131a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (18942, 39)\n",
      "Test Shape  : (1077, 39)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH_PROCESSED)\n",
    "test = pd.read_csv(TEST_PATH_PROCESSED)\n",
    "\n",
    "print('Train shape :', train.shape)\n",
    "print('Test Shape  :', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "46362c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preserve original dataset, we copy just to give some demonstration how these feature engineering works\n",
    "train_lore = train.copy()\n",
    "test_lore = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56120c",
   "metadata": {},
   "source": [
    "# ‚å®Ô∏è **Imputer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5b6b9",
   "metadata": {},
   "source": [
    "Before applying any feature engineering, the dataset shows several missing values across key raw variables, including `GHI`, `DHI`, `DNI`, the clearsky radiation components, and the `Cloud Type` field. These measurements form the foundation for all subsequent analysis, meaning we must restore their completeness before constructing any derived features or transformations. Missing values in this stage do not yet break engineered features‚Äîbut they do disrupt the underlying physical continuity of the solar and weather signals, which would lead to unreliable or biased feature construction later on.\n",
    "\n",
    "By identifying these gaps up front, the imputation stage ensures that the feature-engineering pipeline receives clean, consistent, and physically coherent inputs. This early correction prevents NaNs from propagating into downstream steps such as dynamic lag calculations, weather clustering, or any astronomy- or physics-based transformations that rely on uninterrupted numeric sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11b5f5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test initial missing values:\n",
      "\n",
      "              train_missing  test_missing\n",
      "% Baseline                0          1077\n",
      "moonrise                629            34\n",
      "moonset                 639            33\n",
      "DHI                    1044           167\n",
      "DNI                    1044           167\n",
      "GHI                    1044           167\n",
      "Clearsky DHI           1044           167\n",
      "Clearsky DNI           1044           167\n",
      "Clearsky GHI           1044           167\n",
      "Cloud Type              977           112\n"
     ]
    }
   ],
   "source": [
    "missing_df = pd.DataFrame({\n",
    "    'train_missing': train_lore.isna().sum(),\n",
    "    'test_missing':  test_lore.isna().sum()\n",
    "})\n",
    "\n",
    "missing_df = missing_df[(missing_df['train_missing'] > 0) | (missing_df['test_missing'] > 0)]\n",
    "\n",
    "print('Train test initial missing values:\\n\\n', missing_df, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a02c30",
   "metadata": {},
   "source": [
    "# **Moon Imputer**\n",
    "\n",
    "he `moonrise` and `moonset` columns originally contained only time-of-day strings (e.g., `\"05:12 AM\"`) or placeholders like `\"No moonset\"`, but during feature engineering we converted them into full datetime values by attaching the corresponding year, month, and day. Entries labeled `\"No moonrise\"` or `\"No moonset\"` cannot be parsed into valid timestamps, causing them to become `NaT` and introducing unwanted missing values into the dataset. To prevent this parsing failure, we explicitly replace missing entries with the same placeholder strings before datetime conversion, ensuring consistency and avoiding unintended `NaT` values during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac0968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lore['moonrise'] = train_lore['moonrise'].fillna(\"No moonrise\")\n",
    "train_lore['moonset']  = train_lore['moonset'].fillna(\"No moonset\")\n",
    "\n",
    "test_lore['moonrise'] = test_lore['moonrise'].fillna(\"No moonrise\")\n",
    "test_lore['moonset']  = test_lore['moonset'].fillna(\"No moonset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d22a1b",
   "metadata": {},
   "source": [
    "## **Solar Imputer**\n",
    "\n",
    "Since `GHI`, `DHI`, and `DNI` are continuous solar-irradiance measurements that follow smooth temporal patterns, we impute their missing values using a time-aware strategy. After converting `Timestamp` into a proper datetime index, we apply `interpolate(method='time')` to estimate missing points based on their chronological neighbors, which aligns with the physical behavior of irradiance throughout the day. Any remaining gaps‚Äîsuch as those at the beginning or end of the series‚Äîare then filled using forward and backward fill (`ffill` and `bfill`). This approach preserves the natural temporal structure of the data while ensuring that downstream features relying on these variables remain consistent and free of NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c06aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lore['Timestamp'] = pd.to_datetime(train_lore['Timestamp'], errors='coerce')\n",
    "train_lore = train_lore.set_index('Timestamp')\n",
    "\n",
    "test_lore['Timestamp'] = pd.to_datetime(test_lore['Timestamp'], errors='coerce')\n",
    "test_lore = test_lore.set_index('Timestamp')\n",
    "\n",
    "train_lore[['GHI', 'DHI', 'DNI']] = (\n",
    "    train_lore[['GHI', 'DHI', 'DNI']].interpolate(method='time')\n",
    ")\n",
    "train_lore[['GHI', 'DHI', 'DNI']] = train_lore[['GHI','DHI','DNI']].ffill().bfill()\n",
    "\n",
    "test_lore[['GHI', 'DHI', 'DNI']] = (\n",
    "    test_lore[['GHI', 'DHI', 'DNI']].interpolate(method='time')\n",
    ")\n",
    "test_lore[['GHI', 'DHI', 'DNI']] = test_lore[['GHI','DHI','DNI']].ffill().bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856bab7",
   "metadata": {},
   "source": [
    "## **Clearsky Imputer**\n",
    "\n",
    "For the clearsky radiation components (`Clearsky DHI`, `Clearsky DNI`, and `Clearsky GHI`), we use a `KNNImputer` because these variables exhibit strong correlations with each other and behave in smooth, physically consistent patterns. Unlike simple mean or median replacement, KNN considers the relationships between multiple features and imputes missing values based on the nearest valid observations in the feature space. This allows the imputed values to better reflect realistic atmospheric conditions instead of producing overly generic or biased estimates. By using distance-weighted neighbors, the imputer assigns more influence to samples that are physically similar, resulting in smoother and more reliable clearsky estimates for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ea2e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNImputer(n_neighbors=3, weights='distance')\n",
    "solar_cols = ['Clearsky DHI', 'Clearsky DNI', 'Clearsky GHI']\n",
    "\n",
    "train_lore[solar_cols] = knn.fit_transform(train_lore[solar_cols])\n",
    "test_lore[solar_cols] = knn.fit_transform(test_lore[solar_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7171a",
   "metadata": {},
   "source": [
    "After imputing the solar and clearsky components, we recompute `clearsky_index` and `diffuse_fraction` to ensure these derived ratios remain physically consistent. These features are simple yet informative indicators of atmospheric clarity and radiation scattering, and recalculating them after imputation guarantees that they accurately reflect the corrected underlying measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff4b7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "train_lore['clearsky_index'] = train_lore['GHI'] / (train_lore['Clearsky GHI'] + eps)\n",
    "train_lore['diffuse_fraction'] = train_lore['DHI'] / (train_lore['GHI'] + eps)\n",
    "\n",
    "test_lore['clearsky_index'] = test_lore['GHI'] / (test_lore['Clearsky GHI'] + eps)\n",
    "test_lore['diffuse_fraction'] = test_lore['DHI'] / (test_lore['GHI'] + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc7d65",
   "metadata": {},
   "source": [
    "Missing values in `Cloud Type` are filled with `1` to represent the ‚ÄúUnknown‚Äù category, ensuring all observations retain a valid and consistent cloud classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "454de80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lore['Cloud Type'] = train_lore['Cloud Type'].fillna(1)\n",
    "test_lore['Cloud Type'] = test_lore['Cloud Type'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "add9ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lore.reset_index(inplace=True)\n",
    "test_lore.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64e65b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test initial missing values:\n",
      "\n",
      "            train_missing  test_missing\n",
      "% Baseline              0          1077\n"
     ]
    }
   ],
   "source": [
    "missing_df = pd.DataFrame({\n",
    "    'train_missing': train_lore.isna().sum(),\n",
    "    'test_missing':  test_lore.isna().sum()\n",
    "})\n",
    "\n",
    "missing_df = missing_df[(missing_df['train_missing'] > 0) | (missing_df['test_missing'] > 0)]\n",
    "\n",
    "print('Train test initial missing values:\\n\\n', missing_df, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f01528",
   "metadata": {},
   "source": [
    "With all missing values now resolved and the raw inputs restored to a consistent state, the dataset is ready for the next stage. Since `% Baseline` is the only remaining missing field in the test set and represents our target variable, we can proceed to apply the full feature engineering pipeline on the cleaned train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa5fe5",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a27a7",
   "metadata": {},
   "source": [
    "## **Cloud Type Mapping**\n",
    "\n",
    "From exploration before, we found that our only categorical features is ordinal and has an order. Sorted by Q3, we found that `Cloud Type` fog is producing more energy than `Probably Clear` which seems like an anomaly in terms of real life. But we'll make a data driven decision, so lets map `Cloud Type` with this order:\n",
    "\n",
    "<p float=\"left\">\n",
    "  <center>\n",
    "  <img src=\"../reports/figures/astronomical/cloudtype_impact.png\" width=\"60%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3dd2dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_map = {\n",
    "    'Unknown': 1, 'Opaque Ice': 2, 'Overlapping': 3,\n",
    "    'Super-Cooled Water': 4, 'Cirrus': 5, 'Fog': 6,\n",
    "    'Water': 7, 'Overshooting': 8,\n",
    "    'Probably Clear': 9, 'Clear': 10\n",
    "}\n",
    "\n",
    "train_lore['Cloud Type'] = train_lore['Cloud Type'].map(cloud_map)\n",
    "test_lore['Cloud Type'] = test_lore['Cloud Type'].map(cloud_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334f5f4",
   "metadata": {},
   "source": [
    "## **Time Features**\n",
    "\n",
    "From earlier exploration, we saw that the raw `Timestamp` column contains repeating daily and monthly cycles that models cannot naturally learn from linear time values. To preserve these cyclical patterns, we convert hour and month components into `hour_sin`, `hour_cos`, `month_sin`, and `month_cos`, allowing the model to recognize periodic relationships such as midnight being close to early morning. We also shift time by ‚àí5 hours to better align the solar peak with true solar noon.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "  <img src=\"..\\reports\\figures\\astronomical\\diurnal_cycle.png\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "Additionally, we extract `doy` (day-of-year) to capture broader seasonal trends that influence solar irradiance throughout the year. These engineered time features provide a more physically meaningful representation of temporal behavior, enabling the model to interpret solar patterns more effectively than using the raw timestamp alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d0dd3b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after time features : (18942, 46)\n",
      "Test shape after time features  : (1077, 46)\n"
     ]
    }
   ],
   "source": [
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize timestamp\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "    df = df.sort_values('Timestamp')\n",
    "\n",
    "    # Shift time -5h (solar noon correction)\n",
    "    solar_time = df['Timestamp'] - pd.Timedelta(hours=5)\n",
    "    solar_hour = solar_time.dt.hour\n",
    "\n",
    "    # Cyclical hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * solar_hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * solar_hour / 24)\n",
    "\n",
    "    # Cyclical month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['Timestamp'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['Timestamp'].dt.month / 12)\n",
    "\n",
    "    # Day-of-year (for astronomy)\n",
    "    df['doy'] = solar_time.dt.dayofyear\n",
    "\n",
    "    return df\n",
    "\n",
    "train_time = add_time_features(train_lore)\n",
    "test_time = add_time_features(test_lore)\n",
    "\n",
    "print('Train shape after time features :', train_time.shape)\n",
    "print('Test shape after time features  :', test_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9d668",
   "metadata": {},
   "source": [
    "## **Astronomical Features**\n",
    "\n",
    "Building on the `doy` feature, we introduce astronomy-based variables that reflect the physical behavior of the Sun throughout the year. Using day-of-year, we compute `solar_declination`, which represents the Sun‚Äôs angle relative to the equatorial plane and directly affects the intensity of solar radiation received at the surface. We also add the `equation_of_time`, a small correction that captures irregularities in solar time caused by Earth‚Äôs orbital mechanics.\n",
    "\n",
    "To account for changes in Earth‚ÄìSun distance, we compute `sun_earth_distance_factor` and derive `extraterrestrial_radiation` as a theoretical maximum solar input. These astronomy-driven features ground the model in physical reality, giving it access to seasonal solar dynamics that raw weather data cannot fully represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "173bfeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after astronomy features : (18942, 50)\n",
      "Test shape after astronomy features  : (1077, 50)\n"
     ]
    }
   ],
   "source": [
    "def add_astronomy_features(df):\n",
    "    df = df.copy()\n",
    "    doy = df['doy']\n",
    "\n",
    "    # Solar declination\n",
    "    delta = 23.45 * np.sin(np.radians(360 * (284 + doy) / 365))\n",
    "    df['solar_declination'] = delta\n",
    "\n",
    "    # Equation of Time\n",
    "    B = np.radians((doy - 81) * 360 / 365)\n",
    "    df['equation_of_time'] = (\n",
    "        9.87 * np.sin(2*B) - 7.53 * np.cos(B) - 1.5 * np.sin(B)\n",
    "    )\n",
    "\n",
    "    # Earth-Sun distance\n",
    "    df['sun_earth_distance_factor'] = 1 + 0.033 * np.cos(np.radians(360 * doy / 365))\n",
    "    df['extraterrestrial_radiation'] = 1367 * df['sun_earth_distance_factor']\n",
    "\n",
    "    return df\n",
    "\n",
    "train_astro = add_astronomy_features(train_time)\n",
    "test_astro = add_astronomy_features(test_time)\n",
    "\n",
    "print('Train shape after astronomy features :', train_astro.shape)\n",
    "print('Test shape after astronomy features  :', test_astro.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d6763",
   "metadata": {},
   "source": [
    "## **Sun Features**\n",
    "\n",
    "To incorporate sun-position context, we extract two simple yet meaningful features from the raw `sunrise` and `sunset` columns. First, we compute `sunHour`, representing the total daylight duration for each day. Then we derive `is_daytime`, a binary flag indicating whether a given timestamp falls between sunrise and sunset. These features help the model distinguish between naturally dark hours and hours where solar energy is physically possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9ec94969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after sun features : (18942, 51)\n",
      "Test shape after sun features  : (1077, 51)\n"
     ]
    }
   ],
   "source": [
    "def add_sun_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    sunrise_dt = pd.to_datetime(df['sunrise'])\n",
    "    sunset_dt = pd.to_datetime(df['sunset'])\n",
    "\n",
    "    # length of daylight\n",
    "    df['sunHour'] = (sunset_dt - sunrise_dt).dt.total_seconds()\n",
    "\n",
    "    # day/night flag\n",
    "    curr = df['Timestamp'].dt.time\n",
    "    rise = sunrise_dt.dt.time\n",
    "    set_  = sunset_dt.dt.time\n",
    "\n",
    "    df['is_daytime'] = [\n",
    "        1 if (r <= c <= s) else 0\n",
    "        for c, r, s in zip(curr, rise, set_)\n",
    "    ]\n",
    "\n",
    "    return df\n",
    "\n",
    "train_sun = add_sun_features(train_astro)\n",
    "test_sun = add_sun_features(test_astro)\n",
    "\n",
    "print('Train shape after sun features :', train_sun.shape)\n",
    "print('Test shape after sun features  :', test_sun.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d085e",
   "metadata": {},
   "source": [
    "## **Physics Features**\n",
    "\n",
    "To capture physically meaningful relationships in solar production, we derive features grounded in solar and atmospheric physics. The `clearsky_index` measures how actual irradiance compares to an ideal clear-sky condition, while `diffuse_fraction` reflects the portion of radiation scattered by clouds or aerosols. We also compute `wind_cooling_potential` to approximate how wind and temperature together influence panel efficiency. These derived ratios provide the model with clearer signals than raw GHI or weather values alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "06b02267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after physics features : (18942, 52)\n",
      "Test shape after physics features  : (1077, 52)\n"
     ]
    }
   ],
   "source": [
    "def add_physics_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "\n",
    "    df['clearsky_index'] = df['GHI'] / (df['Clearsky GHI'] + eps)\n",
    "    df['diffuse_fraction'] = df['DHI'] / (df['GHI'] + eps)\n",
    "\n",
    "    # wind cooling using Kelvin\n",
    "    df['wind_cooling_potential'] = df['windspeedKmph'] / (df['tempC'] + 273.15)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_physics = add_physics_features(train_sun)\n",
    "test_physics = add_physics_features(test_sun)\n",
    "\n",
    "print('Train shape after physics features :', train_physics.shape)\n",
    "print('Test shape after physics features  :', test_physics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c7278",
   "metadata": {},
   "source": [
    "## **Rolling Features**\n",
    "\n",
    "To capture short-term temporal behavior in solar conditions, we introduce time-aware dynamic features based on lagged and rolling information. Using each row‚Äôs timestamp, we construct a 1-hour lookup to retrieve the previous values of `GHI` and `cloudcover`, producing `GHI_lag1` and `cloudcover_lag1`. Unlike a simple `.shift()`, this merge-based approach ensures correctness even when timestamps are irregular or contain gaps.\n",
    "\n",
    "We also compute a `GHI_rolling_mean_3h`, which summarizes irradiance over the previous three hours and smooths out rapid fluctuations caused by changing cloud patterns. Together, these dynamic features allow the model to track short-term trends and transitions, something that cannot be captured from static weather snapshots alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "14d9bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after rolling features : (18942, 55)\n",
      "Test shape after rolling features  : (1077, 55)\n"
     ]
    }
   ],
   "source": [
    "def add_time_dynamic_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # lag 1h reference timestamp\n",
    "    df['target_time_1h'] = df['Timestamp'] - pd.Timedelta(hours=1)\n",
    "\n",
    "    # lookup table\n",
    "    lookup = df[['Timestamp', 'GHI', 'cloudcover']].copy()\n",
    "    lookup.columns = ['ts_ref', 'GHI_lag1', 'cloudcover_lag1']\n",
    "\n",
    "    df = df.merge(lookup, left_on='target_time_1h',\n",
    "                  right_on='ts_ref', how='left')\n",
    "\n",
    "    # rolling 3-hour mean\n",
    "    idx = df.set_index('Timestamp')\n",
    "    df['GHI_rolling_mean_3h'] = (\n",
    "        idx['GHI'].rolling('3h', min_periods=1).mean().values\n",
    "    )\n",
    "\n",
    "    # cleanup + fill\n",
    "    df.drop(columns=['target_time_1h', 'ts_ref'], inplace=True)\n",
    "    df[['GHI_lag1','cloudcover_lag1','GHI_rolling_mean_3h']] = \\\n",
    "        df[['GHI_lag1','cloudcover_lag1','GHI_rolling_mean_3h']].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_roll = add_time_dynamic_features(train_physics)\n",
    "test_roll = add_time_dynamic_features(test_physics)\n",
    "\n",
    "print('Train shape after rolling features :', train_roll.shape)\n",
    "print('Test shape after rolling features  :', test_roll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95673a5c",
   "metadata": {},
   "source": [
    "## **Gradient Features**\n",
    "\n",
    "To capture rapid changes in solar and weather conditions, we introduce gradient-based features that quantify how key variables evolve over time. `GHI_diff_1h`, `cloudcover_diff_1h`, and `clearsky_index_diff` measure first-order changes from one hour to the next, while `GHI_acceleration` represents the second derivative, highlighting sudden shifts in irradiance. These temporal gradients help the model detect transitions‚Äîsuch as clouds moving in or out‚Äîthat strongly influence short-term solar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2a5b6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after gradient features : (18942, 59)\n",
      "Test shape after gradient features  : (1077, 59)\n"
     ]
    }
   ],
   "source": [
    "def add_gradient_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['GHI_diff_1h'] = df['GHI'] - df['GHI_lag1']\n",
    "    df['cloudcover_diff_1h'] = df['cloudcover'] - df['cloudcover_lag1']\n",
    "    df['clearsky_index_diff'] = df['clearsky_index'].diff().fillna(0)\n",
    "\n",
    "    # 2nd derivative (acceleration)\n",
    "    df['GHI_acceleration'] = df['GHI_diff_1h'].diff().fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_grad = add_gradient_features(train_roll)\n",
    "test_grad = add_gradient_features(test_roll)\n",
    "\n",
    "print('Train shape after gradient features :', train_grad.shape)\n",
    "print('Test shape after gradient features  :', test_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e2a8a",
   "metadata": {},
   "source": [
    "## **Weather Clustering**\n",
    "\n",
    "To capture broader weather regimes rather than relying solely on raw meteorological values, we cluster historical conditions using KMeans. The clustering is based on short-term irradiance dynamics (`GHI_rolling_mean_3h`), atmospheric properties (`humidity`, `diffuse_fraction`), and cloud-related signals (`cloudcover_lag1`, `clearsky_index`, `windspeedKmph`). The resulting `weather_cluster` assigns each timestamp to a learned weather state, helping the model recognize patterns such as clear-sky conditions, scattered clouds, or rapidly changing environments.\n",
    "\n",
    "To avoid data leakage, the clustering model is fit **only on the training set**, ensuring that the structure of weather regimes comes entirely from past data. Once fitted, this model is applied to both training and test sets using identical feature columns and transformations. This guarantees that the test data is assigned to pre-learned clusters without influencing how those clusters were originally formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "84bf611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after weather clustering : (18942, 60)\n",
      "Test shape after weather clustering  : (1077, 60)\n"
     ]
    }
   ],
   "source": [
    "def fit_weather_clusters(train_df, n_clusters=4):\n",
    "    features = [\n",
    "        'cloudcover_lag1',\n",
    "        'humidity',\n",
    "        'windspeedKmph',\n",
    "        'GHI_rolling_mean_3h',\n",
    "        'clearsky_index',\n",
    "        'diffuse_fraction'\n",
    "    ]\n",
    "\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    model.fit(train_df[features].fillna(0))\n",
    "\n",
    "    return model, features\n",
    "\n",
    "def apply_weather_clusters(df, model, features):\n",
    "    df = df.copy()\n",
    "    df['weather_cluster'] = model.predict(df[features].fillna(0))\n",
    "    return df\n",
    "\n",
    "\n",
    "cluster_model, cluster_feats = fit_weather_clusters(train_grad)\n",
    "\n",
    "train_cluster = apply_weather_clusters(train_grad, cluster_model, cluster_feats)\n",
    "test_cluster  = apply_weather_clusters(test_grad, cluster_model, cluster_feats)\n",
    "\n",
    "print('Train shape after weather clustering :', train_cluster.shape)\n",
    "print('Test shape after weather clustering  :', test_cluster.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc836d9",
   "metadata": {},
   "source": [
    "## **STL Decomposition Features**\n",
    "\n",
    "To isolate different temporal components of solar irradiance, we apply STL decomposition to the training set‚Äôs `GHI` values. This produces three signals such as `trend`, `seasonal`, and `residual` which reflect long-term behavior, daily cyclical patterns, and short-term irregularities, respectively. Because STL itself cannot be directly applied to unseen data, we fit simple linear models on each extracted component to approximate how these signals evolve over time.\n",
    "\n",
    "To prevent leakage, the STL decomposition and the linear models are fit exclusively on the training data. The fitted models are then used to generate `GHI_trend`, `GHI_seasonal`, and `GHI_residual` for both the training and test sets using only their timestamps. This ensures the test set never influences how the decomposition was learned while still benefiting from the same temporal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6fc4bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after STL features : (18942, 63)\n",
      "Test shape after STL features  : (1077, 63)\n"
     ]
    }
   ],
   "source": [
    "def fit_stl_decomposition(train_df, period=24):\n",
    "    # Fit STL on train\n",
    "    stl = STL(train_df['GHI'].fillna(0), period=period).fit()\n",
    "\n",
    "    train_trend = stl.trend\n",
    "    train_seasonal = stl.seasonal\n",
    "    train_resid = stl.resid\n",
    "\n",
    "    # Fit linear models to approximate patterns for test set\n",
    "    t = np.arange(len(train_df)).reshape(-1, 1)\n",
    "\n",
    "    trend_model = LinearRegression().fit(t, train_trend)\n",
    "    seasonal_model = LinearRegression().fit(t, train_seasonal)\n",
    "    resid_model = LinearRegression().fit(t, train_resid)\n",
    "\n",
    "    return trend_model, seasonal_model, resid_model\n",
    "\n",
    "def apply_stl_features(df, trend_model, seasonal_model, resid_model):\n",
    "    df = df.copy()\n",
    "    t = np.arange(len(df)).reshape(-1, 1)\n",
    "\n",
    "    df['GHI_trend'] = trend_model.predict(t)\n",
    "    df['GHI_seasonal'] = seasonal_model.predict(t)\n",
    "    df['GHI_residual'] = resid_model.predict(t)\n",
    "\n",
    "    return df\n",
    "\n",
    "trend_m, season_m, resid_m = fit_stl_decomposition(train_cluster)\n",
    "\n",
    "train_stl = apply_stl_features(train_cluster, trend_m, season_m, resid_m)\n",
    "test_stl  = apply_stl_features(test_cluster, trend_m, season_m, resid_m)\n",
    "\n",
    "print('Train shape after STL features :', train_stl.shape)\n",
    "print('Test shape after STL features  :', test_stl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442981d",
   "metadata": {},
   "source": [
    "## **Drop Unnecessary Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0e01a",
   "metadata": {},
   "source": [
    "Before modeling, we remove all remaining datetime-related columns (`Timestamp`, `sunrise`, `sunset`, `moonrise`, `moonset`) since they cannot be used directly by tree-based models. These features have already been transformed into numerical representations during feature engineering, so dropping the raw columns ensures the final dataset contains only numeric, model-ready inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e50d2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train final shape : (18942, 58)\n",
      "Test final shape  : (1077, 58)\n"
     ]
    }
   ],
   "source": [
    "train_final = train_stl.drop(columns=['Timestamp', 'moonrise', 'moonset', 'sunrise', 'sunset'])\n",
    "test_final = test_stl.drop(columns=['Timestamp', 'moonrise', 'moonset', 'sunrise', 'sunset'])\n",
    "\n",
    "print('Train final shape :', train_final.shape)\n",
    "print('Test final shape  :', test_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b86b6",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è **Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264d9b9",
   "metadata": {},
   "source": [
    "`SolarImputer` cleans and imputes the raw data (fit on train, applied to both train and test), while `FeatureEngineering` learns patterns such as clusters and STL components from the training set and transforms both datasets consistently. This produces final engineered features ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "94487642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train engineered shape : (18942, 58)\n",
      "Test engineered shape  : (1077, 58)\n"
     ]
    }
   ],
   "source": [
    "imputer  = preprocessing.SolarImputer()\n",
    "fe       = preprocessing.FeatureEngineering(n_clusters=4, stl_period=24)\n",
    "\n",
    "# Fit only on train\n",
    "train_clean  = imputer.fit_transform(train)\n",
    "test_clean   = imputer.transform(test)\n",
    "\n",
    "fe.fit(train_clean)\n",
    "train_engineered = fe.transform(train_clean)\n",
    "test_engineered  = fe.transform(test_clean)\n",
    "\n",
    "print('Train engineered shape :', train_engineered.shape)\n",
    "print('Test engineered shape  :', test_engineered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f3afcd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test initial missing values:\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [train_missing, test_missing]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "missing_df = pd.DataFrame({\n",
    "    'train_missing': train_engineered.isna().sum(),\n",
    "    'test_missing':  train_engineered.isna().sum()\n",
    "})\n",
    "\n",
    "missing_df = missing_df[(missing_df['train_missing'] > 0) | (missing_df['test_missing'] > 0)]\n",
    "\n",
    "print('Train test initial missing values:\\n\\n', missing_df, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555cc22",
   "metadata": {},
   "source": [
    "We export the engineered train and test datasets so we don‚Äôt need to rerun the entire preprocessing pipeline (imputation + feature engineering) every time we experiment with models. Saving these files makes the workflow faster, ensures reproducibility, and allows us to reuse the exact same processed features for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7432b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test engineered saved to ..\\data\\processed\\train_engineered.csv and ..\\data\\processed\\test_engineered.csv\n"
     ]
    }
   ],
   "source": [
    "train_engineered.to_csv(TRAIN_PATH_ENGINEERED, index=False)\n",
    "test_engineered.to_csv(TEST_PATH_ENGINEERED, index=False)\n",
    "\n",
    "print(f'Train and Test engineered saved to {TRAIN_PATH_ENGINEERED} and {TEST_PATH_ENGINEERED}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
